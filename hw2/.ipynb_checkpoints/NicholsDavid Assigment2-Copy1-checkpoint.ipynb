{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#github link\n",
    "#https://github.com/DavidN0809/ML-Biomed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10707,
     "status": "ok",
     "timestamp": 1674672885564,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "boK8-kDTHzp4",
    "outputId": "bb710ca8-7529-44fb-d32b-5ef03b874fe9"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1674672885566,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "_FkpYL4BJFx3"
   },
   "outputs": [],
   "source": [
    "#!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1674672885569,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "r9CKJwOOKkGJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plotter_lib\n",
    "\n",
    "pin_memory=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1674672885571,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "f-nJ7E91LiFd",
    "outputId": "06cd25df-7e6a-4479-8042-d386f6599809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "cuda\n",
      "9.2.0\n",
      "9.2.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(torch.cuda.get_device_name(cuda_id))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#pil verison fix\n",
    "import PIL\n",
    "\n",
    "from PIL import __version__\n",
    "print(__version__)\n",
    "\n",
    "from PIL import __version__\n",
    "PIL.PILLOW_VERSION = __version__\n",
    "\n",
    "print(PIL.PILLOW_VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1674672885572,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "fhYewn4ULjX5",
    "outputId": "6a18e4c2-c235-4276-f895-dd6b6436438c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1+cu117'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"/home/david/Documents/GitHub/ML-Biomed/hw1/\")\n",
    "image_path = data_path / \"Data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data'.\n",
      "There are 2 directories and 0 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data/test'.\n",
      "There are 0 directories and 355 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data/test/DRUSEN'.\n",
      "There are 0 directories and 575 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data/test/DME'.\n",
      "There are 2 directories and 0 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data/train'.\n",
      "There are 0 directories and 8265 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data/train/DRUSEN'.\n",
      "There are 0 directories and 10847 images in '/home/david/Documents/GitHub/ML-Biomed/hw1/Data/train/DME'.\n"
     ]
    }
   ],
   "source": [
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/david/Documents/GitHub/ML-Biomed/hw1/Data/train'),\n",
       " PosixPath('/home/david/Documents/GitHub/ML-Biomed/hw1/Data/test'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup train and testing paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target directory: /home/david/Documents/GitHub/ML-Biomed/hw1/Data/train\n",
      "Class names found: ['DME', 'DRUSEN']\n"
     ]
    }
   ],
   "source": [
    "# Setup path for target directory\n",
    "target_directory = train_dir\n",
    "print(f\"Target directory: {target_directory}\")\n",
    "\n",
    "# Get the class names from the target directory\n",
    "class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\n",
    "print(f\"Class names found: {class_names_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# Make function to find classes in target directory\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    \n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['DME', 'DRUSEN'], {'DME': 0, 'DRUSEN': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_classes(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom dataset class (inherits from torch.utils.data.Dataset)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpeg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, class_idx # return data, label (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment train data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Don't augment test data, only reshape\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.ImageFolderCustom at 0x7fab41a25ac0>,\n",
       " <__main__.ImageFolderCustom at 0x7fab41a25bb0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_custom = ImageFolderCustom(targ_dir=train_dir, transform=train_transforms)\n",
    "test_data_custom = ImageFolderCustom(targ_dir=test_dir, transform=test_transforms)\n",
    "train_data_custom, test_data_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup batch size and number of workers \n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "#loading all images from the train and test directorys\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    ImageFolderCustom(targ_dir=train_dir, transform=train_transforms),\n",
    "    'test': \n",
    "    ImageFolderCustom(targ_dir=test_dir, transform=test_transforms)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# create a validation set from the test set\n",
    "val_size = int(len(image_datasets['test']) * 0.1) # use 10% of the test set for validation\n",
    "test_size = len(image_datasets['test']) - val_size\n",
    "\n",
    "test_dataset, val_dataset = random_split(image_datasets['test'], [test_size, val_size])\n",
    "\n",
    "#creating dataloaders\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=NUM_WORKERS),  \n",
    "    'test':\n",
    "    torch.utils.data.DataLoader(test_dataset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=False,\n",
    "                                num_workers=NUM_WORKERS), \n",
    "    'val':\n",
    "    torch.utils.data.DataLoader(val_dataset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=False,\n",
    "                                num_workers=NUM_WORKERS)\n",
    "}\n",
    "\n",
    "#appending val for use in mode\n",
    "image_datasets['val'] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([32, 1, 224, 224]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Get image and label from custom DataLoader\n",
    "img_custom, label_custom = next(iter(dataloaders['val']))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label_custom.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1674672885594,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "1_L6EnDjWY5u"
   },
   "outputs": [],
   "source": [
    "#%cd /content/drive/MyDrive/Github/\n",
    "#!git clone https://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@github.com/DavidN0809/ML-Biomed.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1674672885595,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "G97--tVKWmJA",
    "outputId": "a34b17de-8386-48d6-c203-e032925bcb08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "#checking if gpu is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1192,
     "status": "ok",
     "timestamp": 1674672886708,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "8jg2OQFMW6OU",
    "outputId": "cc3681fd-12ec-4109-b839-4b649112cdaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/david/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#loading resnet50 model\n",
    "model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True #unfreezing model   \n",
    "\n",
    "#adding relu and linear layers \n",
    "model.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 2)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1674672887013,
     "user": {
      "displayName": "David Nichols",
      "userId": "03285782024896652237"
     },
     "user_tz": 300
    },
    "id": "ZOXNbqU_W-X2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david/Documents/GitHub/ML-Biomed/hw2\n"
     ]
    }
   ],
   "source": [
    "# Lists to store values for plotting\n",
    "acc_history = []\n",
    "loss_history = []\n",
    "\n",
    "import datetime\n",
    "\n",
    "#%cd C:/Users/alway/Documents/GitHub/ML-Biomed/hw1/\n",
    "#%cd /dmc/ML-Biomed/hw1/\n",
    "%cd /home/david/Documents/GitHub/ML-Biomed/hw2\n",
    "\n",
    "def train_model(model, num_epochs=3, early_stop_patience=5, lr=1e-5):\n",
    "    # Define the cross-entropy loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define the optimizer using Adam and the specified learning rate\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize the early stopping counter\n",
    "    early_stop_counter = 0\n",
    "    # Initialize the best validation loss and the corresponding model\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = model\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Loop over the training and validation phases\n",
    "        for phase in ['train', 'val']:\n",
    "            # If in training phase, set the model to train mode\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            # If in validation phase, set the model to evaluation mode\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            start_time = datetime.datetime.now()\n",
    "            \n",
    "            # Loop over the data in the dataloader for the current phase\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Move the inputs and labels to the specified device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # If in validation phase, set no_grad\n",
    "                if phase == \"validation\":\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                # Calculate the loss using the criterion\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # If in training phase, perform backpropagation and optimization step\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Get the predictions and calculate the number of correct predictions\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            end_time = datetime.datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            # Calculate the average loss and accuracy for the current epoch\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            # Append the accuracy and loss to the history list\n",
    "            acc_history.append(epoch_acc)\n",
    "            loss_history.append(epoch_loss)\n",
    "\n",
    "\n",
    "            print(f'{phase} loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}, duration: {duration}')\n",
    "            #added model checkpointing\n",
    "            #ModelCheckpoint.save(epoch)\n",
    "            \n",
    "            # check the validation loss\n",
    "            #if phase == 'val':\n",
    "            if epoch_loss < best_val_loss:\n",
    "                # update the best validation loss and the corresponding model\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model = model\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= early_stop_patience:\n",
    "                    epoch += 1\n",
    "                    print(f'Early stopping at epoch: {epoch:.0f}')\n",
    "                    return epoch, best_model\n",
    "    return epoch, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XtvMw6nXBiq",
    "outputId": "bd1a9d53-f1e4-440d-b78b-65669134fa6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 1, 224, 224] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12723/3615946257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mreturned_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12723/2396671045.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, early_stop_patience, lr)\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# Calculate the loss using the criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 1, 224, 224] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "#setting very high epochs so early stop will turn on\n",
    "num_epochs=35\n",
    "#tried 1e-4,1e-5 and 4e-5, 4e-5 got the best accuracy the quickest\n",
    "lr=4e-5\n",
    "#this was chosen due to 2 and 3 stopping too soon, and 5 never stopping\n",
    "early=4\n",
    "\n",
    "returned_epochs, model_trained = train_model(model,num_epochs,lr=lr, early_stop_patience=early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aSNeEpAgAvn"
   },
   "outputs": [],
   "source": [
    "#%cd /content/drive/MyDrive/Github/\n",
    "#%mkdir models\n",
    "%cd C:/Users/alway/Documents/GitHub/ML-Biomed/hw2/\n",
    "%mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlZrLxy6gIxo"
   },
   "outputs": [],
   "source": [
    "#saving model incase we need it later\n",
    "torch.save(model_trained.state_dict(), 'models/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing lists to store train and validation loss and accuracy history\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "train_acc_history = []\n",
    "\n",
    "# Looping through the zip of the loss_history, acc_history\n",
    "# taking every 2nd item to separate train and validation history\n",
    "for i, (train_loss, test_loss, test_acc, train_acc) in enumerate(zip(loss_history[0::2], loss_history[1::2], acc_history[0::2], acc_history[1::2])):\n",
    "    # Appending train loss to the train_loss_history list\n",
    "    train_loss_history.append(train_loss)\n",
    "    # Appending validation loss to the val_loss_history list\n",
    "    val_loss_history.append(test_loss)\n",
    "    # Appending validation accuracy to the val_acc_history list\n",
    "    # converting the value to CPU using .cpu() method\n",
    "    val_acc_history.append(test_acc.cpu())\n",
    "    # Appending train accuracy to the train_acc_history list\n",
    "    # converting the value to CPU using .cpu() method\n",
    "    train_acc_history.append(train_acc.cpu())\n",
    "\n",
    "# Converting the lists to numpy arrays\n",
    "train_loss_history = np.array(train_loss_history)\n",
    "test_loss_history = np.array(val_loss_history)\n",
    "test_acc_history = np.array(val_acc_history)\n",
    "train_acc_history = np.array(train_acc_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzDWiqAbPtRM"
   },
   "outputs": [],
   "source": [
    "#!git remote add origin https://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@github.com/DavidN0809/ML-Biomed.git\n",
    "#!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqFeTO6gQErA"
   },
   "outputs": [],
   "source": [
    "#creating range from epochs for plotting\n",
    "epochs = range(1,returned_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure object with size and resolution specified\n",
    "fig = plt.figure(figsize=(6, 6), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Plot training loss in green color and label it as \"Training loss\"\n",
    "plt.plot(epochs, train_loss_history, 'g', label='Training loss')\n",
    "\n",
    "# Plot validation loss in blue color and label it as \"Validation loss\"\n",
    "plt.plot(epochs, test_loss_history, 'b', label='Validation loss')\n",
    "\n",
    "# Set title for the plot\n",
    "plt.title('Training and Validation loss')\n",
    "\n",
    "# Label x-axis as \"Epochs\"\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Label y-axis as \"Loss\"\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display legend for the plot\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot to a file named 'Loss'\n",
    "plt.savefig('Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy history for training and validation sets\n",
    "fig=plt.figure(figsize=(6,6), dpi= 100, facecolor='w', edgecolor='k') # Create a figure with specified size and background color\n",
    "\n",
    "plt.plot(epochs, train_acc_history, 'g', label='Training loss') # Plot the training accuracy history in green color\n",
    "plt.plot(epochs, test_acc_history, 'b', label='Validation loss') # Plot the validation accuracy history in blue color\n",
    "\n",
    "plt.title('Training and Validation Accuracy') # Set the title of the plot\n",
    "plt.xlabel('Epochs') # Set the x-axis label to 'Epochs'\n",
    "plt.ylabel('Loss') # Set the y-axis label to 'Loss'\n",
    "plt.legend() # Add the legend to the plot\n",
    "plt.savefig(\"Accuracy\") # Save the plot to a file named 'Accuracy'\n",
    "plt.grid() # Add a grid to the plot\n",
    "plt.show() # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_trained.eval()\n",
    "\n",
    "# Initialize variables to store metrics\n",
    "accuracy = 0\n",
    "\n",
    "# Initialize variables to store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate over the validation data\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    # Move inputs and labels to the device\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Pass data through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Compute accuracy\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    accuracy += torch.sum(preds == labels.data)\n",
    "    predictions.append(preds)\n",
    "    true_labels.append(labels)\n",
    "\n",
    "# Concatenate the predictions and true labels into a single array\n",
    "predictions = torch.cat(predictions).cpu()\n",
    "true_labels = torch.cat(true_labels).cpu()\n",
    "\n",
    "# Calculate the average accuracy of the predictions\n",
    "accuracy = accuracy.double() / len(val_dataset)\n",
    "print('Test accuracy: {:.4f}'.format(accuracy))\n",
    "\n",
    "# Compute the confusion matrix of the model\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "index_labels = ['True Label', 'False Label']\n",
    "column_labels = ['Predicted True Label', 'Predicted False Label']\n",
    "df_cm = pd.DataFrame(cm, index=index_labels, columns=column_labels)\n",
    "\n",
    "# Set font size for the annotations in the confusion matrix\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMQHjgv0ns+6tB6YaVlLrz4",
   "mount_file_id": "1mtftlZdfXktuCciH2S5NWhRdrsuJ0yMG",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f146622d2c442757d401e3a576fbb28772c09d1190a48552533281ae0c3386ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
